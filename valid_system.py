#!/usr/bin/env python3
"""
Syst√®me de validation multi-niveaux pour l'enrichissement des ranges
Valide en temps r√©el, par contexte, et globalement
"""

import json
import sqlite3
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

# R√©utilise les classes de l'enrichisseur
from pathlib import Path


# ============================================================================
# VALIDATION MODELS - Structures pour la validation
# ============================================================================

class ValidationLevel(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class ValidationIssue:
    """Un probl√®me de validation d√©tect√©"""
    level: ValidationLevel
    category: str
    message: str
    context_id: Optional[int] = None
    context_name: Optional[str] = None
    field: Optional[str] = None
    suggestion: Optional[str] = None


@dataclass
class ValidationReport:
    """Rapport de validation complet"""
    total_contexts: int
    validated_contexts: int
    issues: List[ValidationIssue]
    global_score: float
    context_scores: Dict[int, float]
    timestamp: str


# ============================================================================
# VALIDATORS - Validateurs sp√©cialis√©s
# ============================================================================

class LogicalValidator:
    """Valide la coh√©rence logique des m√©tadonn√©es"""

    def validate_metadata(self, metadata: Dict, context_name: str, context_id: int) -> List[ValidationIssue]:
        """Valide un contexte et retourne les probl√®mes trouv√©s"""
        issues = []

        # Validation des positions
        issues.extend(self._validate_positions(metadata, context_name, context_id))

        # Validation action-sizing
        issues.extend(self._validate_action_sizing(metadata, context_name, context_id))

        # Validation action-vs_position
        issues.extend(self._validate_action_vs_position(metadata, context_name, context_id))

        # Validation stack depth
        issues.extend(self._validate_stack_depth(metadata, context_name, context_id))

        # Validation format coh√©rence
        issues.extend(self._validate_format_consistency(metadata, context_name, context_id))

        return issues

    def _validate_positions(self, metadata: Dict, context_name: str, context_id: int) -> List[ValidationIssue]:
        """Valide les positions"""
        issues = []

        hero_pos = metadata.get('hero_position')
        vs_pos = metadata.get('vs_position')

        # Position h√©ros obligatoire
        if not hero_pos:
            issues.append(ValidationIssue(
                level=ValidationLevel.ERROR,
                category="position",
                message="Position h√©ros manquante",
                context_id=context_id,
                context_name=context_name,
                field="hero_position",
                suggestion="Sp√©cifier la position du joueur"
            ))

        # H√©ros ‚â† adversaire
        if hero_pos and vs_pos and hero_pos == vs_pos:
            issues.append(ValidationIssue(
                level=ValidationLevel.CRITICAL,
                category="position",
                message=f"Position h√©ros et adversaire identiques ({hero_pos})",
                context_id=context_id,
                context_name=context_name,
                field="vs_position",
                suggestion="V√©rifier qui est le h√©ros et qui est l'adversaire"
            ))

        # V√©rifier ordre logique des positions
        if hero_pos and vs_pos:
            if not self._is_position_order_logical(hero_pos, vs_pos, metadata.get('primary_action')):
                issues.append(ValidationIssue(
                    level=ValidationLevel.WARNING,
                    category="position",
                    message=f"Ordre des positions inhabituel: {hero_pos} vs {vs_pos}",
                    context_id=context_id,
                    context_name=context_name,
                    field="vs_position",
                    suggestion="V√©rifier l'ordre d'action au poker"
                ))

        return issues

    def _validate_action_sizing(self, metadata: Dict, context_name: str, context_id: int) -> List[ValidationIssue]:
        """Valide coh√©rence action-sizing"""
        issues = []

        action = metadata.get('primary_action')
        sizing = metadata.get('sizing')

        # Actions qui ne devraient pas avoir de sizing
        passive_actions = ['call', 'check', 'fold']
        if action in passive_actions and sizing:
            issues.append(ValidationIssue(
                level=ValidationLevel.WARNING,
                category="action",
                message=f"Action '{action}' avec sizing '{sizing}' (inhabituel)",
                context_id=context_id,
                context_name=context_name,
                field="sizing",
                suggestion="Les actions passives n'ont g√©n√©ralement pas de sizing"
            ))

        # Actions qui devraient avoir un sizing
        active_actions = ['open', '3bet', '4bet']
        if action in active_actions and not sizing:
            issues.append(ValidationIssue(
                level=ValidationLevel.INFO,
                category="action",
                message=f"Action '{action}' sans sizing sp√©cifi√©",
                context_id=context_id,
                context_name=context_name,
                field="sizing",
                suggestion="Pr√©ciser le sizing (ex: 2.5x, 3bb)"
            ))

        return issues

    def _validate_action_vs_position(self, metadata: Dict, context_name: str, context_id: int) -> List[ValidationIssue]:
        """Valide coh√©rence action-vs_position"""
        issues = []

        action = metadata.get('primary_action')
        vs_pos = metadata.get('vs_position')

        # Actions r√©actives qui devraient avoir un vs_position
        reactive_actions = ['call', 'defense', '3bet', '4bet']
        if action in reactive_actions and not vs_pos:
            issues.append(ValidationIssue(
                level=ValidationLevel.WARNING,
                category="action",
                message=f"Action r√©active '{action}' sans adversaire sp√©cifi√©",
                context_id=context_id,
                context_name=context_name,
                field="vs_position",
                suggestion="Sp√©cifier contre quelle position cette action est utilis√©e"
            ))

        # Actions initiales qui ne devraient pas avoir de vs_position
        if action == 'open' and vs_pos:
            issues.append(ValidationIssue(
                level=ValidationLevel.INFO,
                category="action",
                message=f"Action 'open' avec vs_position '{vs_pos}' (inhabituel)",
                context_id=context_id,
                context_name=context_name,
                field="vs_position",
                suggestion="L'open est g√©n√©ralement une action initiale"
            ))

        return issues

    def _validate_stack_depth(self, metadata: Dict, context_name: str, context_id: int) -> List[ValidationIssue]:
        """Valide la stack depth"""
        issues = []

        stack = metadata.get('stack_depth')
        action = metadata.get('primary_action')

        # Stack depth obligatoire
        if not stack:
            issues.append(ValidationIssue(
                level=ValidationLevel.WARNING,
                category="stack",
                message="Stack depth non sp√©cifi√©e",
                context_id=context_id,
                context_name=context_name,
                field="stack_depth",
                suggestion="La stack depth influence significativement les ranges"
            ))

        # V√©rifier coh√©rence stack-action
        if stack and action:
            if stack.startswith('20-40bb') and action in ['4bet', '5bet']:
                issues.append(ValidationIssue(
                    level=ValidationLevel.WARNING,
                    category="stack",
                    message=f"Stack courte ({stack}) avec action '{action}' (rare)",
                    context_id=context_id,
                    context_name=context_name,
                    field="stack_depth",
                    suggestion="V√©rifier si cette situation est r√©aliste"
                ))

        return issues

    def _validate_format_consistency(self, metadata: Dict, context_name: str, context_id: int) -> List[ValidationIssue]:
        """Valide la coh√©rence des formats"""
        issues = []

        game_format = metadata.get('game_format')
        variant = metadata.get('variant')
        table_format = metadata.get('table_format')

        # V√©rifier coh√©rence game_format-stack_depth
        stack = metadata.get('stack_depth')
        if game_format == 'tournament' and stack and 'bb' in stack:
            try:
                bb_value = int(stack.replace('bb', '').split('-')[0])
                if bb_value > 200:
                    issues.append(ValidationIssue(
                        level=ValidationLevel.INFO,
                        category="format",
                        message=f"Tournoi avec stack profonde ({stack})",
                        context_id=context_id,
                        context_name=context_name,
                        field="stack_depth",
                        suggestion="V√©rifier si c'est en d√©but de tournoi"
                    ))
            except:
                pass

        return issues

    def _is_position_order_logical(self, hero_pos: str, vs_pos: str, action: str) -> bool:
        """V√©rifie si l'ordre des positions est logique"""
        # Ordre des positions au poker
        position_order = ['UTG', 'UTG+1', 'MP', 'MP+1', 'LJ', 'HJ', 'CO', 'BTN', 'SB', 'BB']

        try:
            hero_idx = position_order.index(hero_pos)
            vs_idx = position_order.index(vs_pos)

            # Pour les actions r√©actives, le h√©ros devrait agir apr√®s l'adversaire
            if action in ['call', '3bet', 'defense']:
                return hero_idx > vs_idx or (hero_pos in ['SB', 'BB'] and vs_pos == 'BTN')

            return True  # Autres cas consid√©r√©s comme OK
        except ValueError:
            return True  # Position non reconnue, pas de validation


class GlobalValidator:
    """Valide la coh√©rence globale entre tous les contextes"""

    def __init__(self, db_path: str):
        self.db_path = db_path

    def validate_global_consistency(self) -> List[ValidationIssue]:
        """Valide la coh√©rence globale"""
        issues = []

        contexts = self._get_all_enriched_contexts()

        if not contexts:
            return issues

        # Validation format global
        issues.extend(self._validate_global_formats(contexts))

        # Validation completeness
        issues.extend(self._validate_completeness(contexts))

        # Validation range coverage
        issues.extend(self._validate_range_coverage(contexts))

        return issues

    def _get_all_enriched_contexts(self) -> List[Dict]:
        """R√©cup√®re tous les contextes enrichis"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT id, name, enriched_metadata 
                FROM range_contexts 
                WHERE json_extract(enriched_metadata, '$.enriched_by_user') = 'true'
            """)

            contexts = []
            for row in cursor.fetchall():
                contexts.append({
                    'id': row[0],
                    'name': row[1],
                    'metadata': json.loads(row[2])
                })
            return contexts

    def _validate_global_formats(self, contexts: List[Dict]) -> List[ValidationIssue]:
        """Valide la coh√©rence des formats globaux"""
        issues = []

        # V√©rifier coh√©rence game_format
        game_formats = [ctx['metadata'].get('game_format') for ctx in contexts]
        unique_formats = set(filter(None, game_formats))

        if len(unique_formats) > 1:
            issues.append(ValidationIssue(
                level=ValidationLevel.WARNING,
                category="global",
                message=f"Formats de jeu mixtes: {', '.join(unique_formats)}",
                suggestion="V√©rifier si c'est intentionnel ou erreur de saisie"
            ))

        # V√©rifier coh√©rence variant
        variants = [ctx['metadata'].get('variant') for ctx in contexts]
        unique_variants = set(filter(None, variants))

        if len(unique_variants) > 1:
            issues.append(ValidationIssue(
                level=ValidationLevel.INFO,
                category="global",
                message=f"Variantes mixtes: {', '.join(unique_variants)}",
                suggestion="Normal si vous jouez plusieurs variantes"
            ))

        return issues

    def _validate_completeness(self, contexts: List[Dict]) -> List[ValidationIssue]:
        """Valide la compl√©tude des m√©tadonn√©es"""
        issues = []

        # Champs essentiels
        essential_fields = ['hero_position', 'primary_action', 'stack_depth']

        for field in essential_fields:
            missing_count = sum(1 for ctx in contexts if not ctx['metadata'].get(field))
            if missing_count > 0:
                percentage = (missing_count / len(contexts)) * 100
                issues.append(ValidationIssue(
                    level=ValidationLevel.WARNING if percentage > 20 else ValidationLevel.INFO,
                    category="completeness",
                    message=f"Champ '{field}' manquant dans {missing_count}/{len(contexts)} contextes ({percentage:.1f}%)",
                    suggestion=f"Compl√©ter le champ '{field}' pour une meilleure qualit√©"
                ))

        return issues

    def _validate_range_coverage(self, contexts: List[Dict]) -> List[ValidationIssue]:
        """Valide la couverture des ranges (d√©tecte les manques)"""
        issues = []

        # Analyser les positions couvertes
        positions = [ctx['metadata'].get('hero_position') for ctx in contexts]
        position_counts = {}
        for pos in positions:
            if pos:
                position_counts[pos] = position_counts.get(pos, 0) + 1

        # D√©tecter positions sous-repr√©sent√©es
        if position_counts:
            avg_count = sum(position_counts.values()) / len(position_counts)
            for pos, count in position_counts.items():
                if count < avg_count * 0.5:  # Moins de 50% de la moyenne
                    issues.append(ValidationIssue(
                        level=ValidationLevel.INFO,
                        category="coverage",
                        message=f"Position '{pos}' sous-repr√©sent√©e ({count} contextes)",
                        suggestion="Consid√©rer ajouter plus de ranges pour cette position"
                    ))

        return issues


# ============================================================================
# VALIDATION MANAGER - Orchestrateur principal
# ============================================================================

class ValidationManager:
    """Gestionnaire principal du syst√®me de validation"""

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.logical_validator = LogicalValidator()
        self.global_validator = GlobalValidator(db_path)

    def validate_single_context(self, context_id: int, metadata: Dict, context_name: str) -> Tuple[
        List[ValidationIssue], float]:
        """Valide un contexte unique"""
        issues = self.logical_validator.validate_metadata(metadata, context_name, context_id)
        score = self._calculate_context_score(metadata, issues)
        return issues, score

    def validate_all_contexts(self) -> ValidationReport:
        """Valide tous les contextes enrichis"""

        # R√©cup√©rer tous les contextes
        contexts = self._get_enriched_contexts()

        all_issues = []
        context_scores = {}

        # Valider chaque contexte
        for ctx in contexts:
            ctx_id = ctx['id']
            ctx_name = ctx['name']
            metadata = ctx['metadata']

            issues, score = self.validate_single_context(ctx_id, metadata, ctx_name)
            all_issues.extend(issues)
            context_scores[ctx_id] = score

        # Validation globale
        global_issues = self.global_validator.validate_global_consistency()
        all_issues.extend(global_issues)

        # Calculer score global
        global_score = self._calculate_global_score(context_scores, global_issues)

        return ValidationReport(
            total_contexts=len(contexts),
            validated_contexts=len([s for s in context_scores.values() if s > 0.5]),
            issues=all_issues,
            global_score=global_score,
            context_scores=context_scores,
            timestamp=datetime.now().isoformat()
        )

    def _get_enriched_contexts(self) -> List[Dict]:
        """R√©cup√®re les contextes enrichis (version robuste corrig√©e)"""
        with sqlite3.connect(self.db_path) as conn:
            # Essayer plusieurs crit√®res pour trouver les contextes enrichis
            queries = [
                # Crit√®re principal: marqu√© comme enrichi par utilisateur (boolean)
                """SELECT id, name, enriched_metadata 
                   FROM range_contexts 
                   WHERE json_extract(enriched_metadata, '$.enriched_by_user') = true""",

                # Crit√®re alternatif: marqu√© comme enrichi (string)
                """SELECT id, name, enriched_metadata 
                   FROM range_contexts 
                   WHERE json_extract(enriched_metadata, '$.enriched_by_user') = 'true'""",

                # Crit√®re: m√©tadonn√©es non vides avec champs essentiels
                """SELECT id, name, enriched_metadata 
                   FROM range_contexts 
                   WHERE enriched_metadata != '{}' 
                   AND enriched_metadata IS NOT NULL 
                   AND enriched_metadata != ''
                   AND json_extract(enriched_metadata, '$.hero_position') IS NOT NULL""",

                # Crit√®re: confiance > 0.5 avec m√©tadonn√©es
                """SELECT id, name, enriched_metadata 
                   FROM range_contexts 
                   WHERE confidence > 0.5
                   AND enriched_metadata != '{}'
                   AND json_extract(enriched_metadata, '$.hero_position') IS NOT NULL"""
            ]

            for i, query in enumerate(queries):
                try:
                    cursor = conn.execute(query)
                    contexts = []

                    for row in cursor.fetchall():
                        try:
                            metadata = json.loads(row[2]) if row[2] else {}
                            # V√©rifier que les m√©tadonn√©es contiennent des donn√©es utiles
                            if metadata and metadata.get('hero_position'):
                                contexts.append({
                                    'id': row[0],
                                    'name': row[1],
                                    'metadata': metadata
                                })
                        except json.JSONDecodeError:
                            continue

                    if contexts:
                        if i > 0:  # Si on a d√ª utiliser un crit√®re alternatif
                            print(f"‚ÑπÔ∏è  Crit√®re {i + 1} utilis√©: {len(contexts)} contextes trouv√©s")
                        return contexts

                except Exception as e:
                    print(f"‚ö†Ô∏è  Erreur requ√™te {i + 1}: {e}")
                    continue

            # Si aucune requ√™te n'a fonctionn√©, retourner liste vide
            print("‚ö†Ô∏è  Aucun contexte enrichi trouv√© avec tous les crit√®res")
            return []

    def _calculate_context_score(self, metadata: Dict, issues: List[ValidationIssue]) -> float:
        """Calcule le score de qualit√© d'un contexte"""
        base_score = 1.0

        # P√©nalit√©s selon le niveau des probl√®mes
        for issue in issues:
            if issue.level == ValidationLevel.CRITICAL:
                base_score -= 0.3
            elif issue.level == ValidationLevel.ERROR:
                base_score -= 0.2
            elif issue.level == ValidationLevel.WARNING:
                base_score -= 0.1
            elif issue.level == ValidationLevel.INFO:
                base_score -= 0.05

        # Bonus pour compl√©tude
        essential_fields = ['hero_position', 'primary_action', 'stack_depth', 'game_format', 'variant']
        completeness = sum(1 for field in essential_fields if metadata.get(field)) / len(essential_fields)

        final_score = max(0.0, base_score * completeness)
        return min(1.0, final_score)

    def _calculate_global_score(self, context_scores: Dict[int, float], global_issues: List[ValidationIssue]) -> float:
        """Calcule le score global"""
        if not context_scores:
            return 0.0

        # Score moyen des contextes
        avg_context_score = sum(context_scores.values()) / len(context_scores)

        # P√©nalit√©s globales
        global_penalty = 0.0
        for issue in global_issues:
            if issue.level == ValidationLevel.CRITICAL:
                global_penalty += 0.2
            elif issue.level == ValidationLevel.ERROR:
                global_penalty += 0.1
            elif issue.level == ValidationLevel.WARNING:
                global_penalty += 0.05

        return max(0.0, avg_context_score - global_penalty)

    def display_validation_report(self, report: ValidationReport):
        """Affiche un rapport de validation format√©"""

        print("\n" + "=" * 60)
        print("üìä RAPPORT DE VALIDATION MULTI-NIVEAUX")
        print("=" * 60)

        # Score global
        score_color = "üü¢" if report.global_score > 0.8 else "üü°" if report.global_score > 0.6 else "üî¥"
        print(f"\n{score_color} SCORE GLOBAL: {report.global_score:.1%}")
        print(f"üìã Contextes valid√©s: {report.validated_contexts}/{report.total_contexts}")

        # R√©sum√© des probl√®mes par niveau
        issues_by_level = {}
        for issue in report.issues:
            level = issue.level.value
            issues_by_level[level] = issues_by_level.get(level, 0) + 1

        if issues_by_level:
            print(f"\n‚ö†Ô∏è  PROBL√àMES D√âTECT√âS:")
            for level, count in sorted(issues_by_level.items()):
                emoji = {"critical": "üî¥", "error": "üü†", "warning": "üü°", "info": "‚ÑπÔ∏è"}.get(level, "‚ùì")
                print(f"   {emoji} {level.title()}: {count}")
        else:
            print(f"\n‚úÖ Aucun probl√®me d√©tect√©!")

        # Top 5 des probl√®mes les plus critiques
        critical_issues = [i for i in report.issues if i.level in [ValidationLevel.CRITICAL, ValidationLevel.ERROR]]
        if critical_issues:
            print(f"\nüî• PROBL√àMES PRIORITAIRES:")
            for i, issue in enumerate(critical_issues[:5], 1):
                print(f"   {i}. {issue.context_name}: {issue.message}")
                if issue.suggestion:
                    print(f"      üí° {issue.suggestion}")

        # Contextes avec scores les plus bas
        low_score_contexts = [(ctx_id, score) for ctx_id, score in report.context_scores.items() if score < 0.7]
        if low_score_contexts:
            print(f"\nüìâ CONTEXTES √Ä REVOIR:")
            low_score_contexts.sort(key=lambda x: x[1])
            for ctx_id, score in low_score_contexts[:3]:
                ctx_name = self._get_context_name(ctx_id)
                print(f"   ‚Ä¢ {ctx_name}: {score:.1%}")

        print(f"\nüïí Rapport g√©n√©r√© le: {report.timestamp}")

    def _get_context_name(self, context_id: int) -> str:
        """R√©cup√®re le nom d'un contexte"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("SELECT name FROM range_contexts WHERE id = ?", (context_id,))
            row = cursor.fetchone()
            return row[0] if row else f"Contexte {context_id}"

    def interactive_validation_review(self):
        """Mode interactif pour r√©viser les probl√®mes de validation"""
        report = self.validate_all_contexts()
        self.display_validation_report(report)

        if not report.issues:
            print("\nüéâ F√©licitations ! Toutes vos ranges sont parfaitement valid√©es.")
            return

        print(f"\nüîß MODE R√âVISION INTERACTIVE")

        # Grouper les probl√®mes par contexte
        issues_by_context = {}
        for issue in report.issues:
            if issue.context_id:
                if issue.context_id not in issues_by_context:
                    issues_by_context[issue.context_id] = []
                issues_by_context[issue.context_id].append(issue)

        # Proposer r√©vision des contextes probl√©matiques
        for ctx_id, issues in issues_by_context.items():
            ctx_name = self._get_context_name(ctx_id)
            score = report.context_scores.get(ctx_id, 0)

            if score < 0.8:  # Seuil de r√©vision
                print(f"\n{'=' * 40}")
                print(f"üìã {ctx_name} (Score: {score:.1%})")
                print(f"{'=' * 40}")

                for issue in issues:
                    level_emoji = {"critical": "üî¥", "error": "üü†", "warning": "üü°", "info": "‚ÑπÔ∏è"}
                    print(f"{level_emoji.get(issue.level.value, '‚ùì')} {issue.message}")
                    if issue.suggestion:
                        print(f"   üí° {issue.suggestion}")

                choice = input("\nüîß Corriger ce contexte ? (o/n/q pour quitter): ").strip().lower()
                if choice == 'q':
                    break
                elif choice == 'o':
                    print("üí° Relancez l'enrichisseur pour corriger ce contexte")


# ============================================================================
# SCRIPT PRINCIPAL
# ============================================================================

def main():
    """Point d'entr√©e pour la validation"""
    print("üîç SYST√àME DE VALIDATION MULTI-NIVEAUX")
    print("=" * 50)

    db_path = "data/poker_trainer.db"

    if not Path(db_path).exists():
        print("‚ùå Base de donn√©es non trouv√©e!")
        print("üí° Lancez d'abord l'import et l'enrichissement")
        return

    validator = ValidationManager(db_path)

    print("Choisissez une option:")
    print("1. Rapport de validation complet")
    print("2. R√©vision interactive des probl√®mes")
    print("3. Validation en continu (mode d√©veloppeur)")

    choice = input("\nVotre choix (1-3): ").strip()

    if choice == "1":
        report = validator.validate_all_contexts()
        validator.display_validation_report(report)

    elif choice == "2":
        validator.interactive_validation_review()

    elif choice == "3":
        print("üîÑ Mode validation continue activ√©")
        print("üí° Cette fonctionnalit√© sera int√©gr√©e √† l'enrichisseur")

    else:
        print("‚ùå Choix invalide")


if __name__ == "__main__":
    main()